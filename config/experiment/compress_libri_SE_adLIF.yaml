# @package _global_

defaults:
  - /dataset: compress_libri

exp_name: compres_libri_SE_adLIF

# Which cell to use
cell: se_adlif

auto_regression: True

# input size 
input_size: 1
# number of neurons
n_neurons: 300
# number of neurons in the big layer
n_neurons_big: 300
# number of neurons of the small layer
n_neurons_small: 100


# Whether to use a two-layer network
two_layers: True

# light decoder is a decoder with only a LI layer
light_decoder: False

# Factor for reparametrization of a and b (see paper)
q: 120

# Parameters of SLAYER
alpha: 5.0
c: 0.4
thr: 1.0
dt: 1.0
# Number of epochs
n_epochs: 200
use_recurrent: True

# Range of tau_u
tau_u_range: [5, 25]

# Range of tau_w
tau_w_range: [60, 300]

# Dropout rate
dropout: 0.0

# Time constant of output layer 
tau_out_range: [1, 20]
train_tau_out_method: 'interpolation'



# Loss aggregation
loss_agg: MSE

# Metric tracking
tracking_metric: val_loss
tracking_mode: min

# optimizer parameters
# learning rate of the optimizer
lr: 0.01
factor: 0.5
patience: 15

# warm start, the lr is fixed at fast_epoch_lr_factor*lr for the first num_fast_epoch
num_fast_epoch: 0
fast_epoch_lr_factor: 10

# weight gain (spread) of ff_weight
ff_gain: 10

# loss parameters
n_mels: 64 # number of bins in the mel-scale spectrogram
min_window: 6 # min size of the FFT windows in power of two 
max_window: 9 # max size of the FTT windows in power of two

# regularization parameters
min_spike_prob: 0.1
max_spike_prob: 0.2
min_layer_coeff: [100.0, 1.0, 1.0, 1.0] # encourage the first layer to spike
max_layer_coeff: [1.0, 1.0, 1.0, 1.0] # here we may encourage lower spike for the second layer to encourage compression


random_seed: 42
