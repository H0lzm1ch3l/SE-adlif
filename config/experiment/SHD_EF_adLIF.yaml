# @package _global_

defaults:
  - /dataset: shd

exp_name: SHD_EF_adLIF

# Whether to use a two-layer network
two_layers: true
# SLAYER parameters
alpha: 5.0
c: 0.4
dt: 1.0
# Dropout rate
dropout: 0.15

l1:
  # Which cell to use
  cell: ef_adlif
# input size of preprocessed SHD
  input_size: 140
  # number of neurons
  n_neurons: 360
  # Range of tau_u
  tau_u_range: [5, 25]
  # Range of tau_w
  tau_w_range: [60, 300]
  # Factor for reparametrization of a and b (see paper)
  q: 60
  alpha: ${alpha}
  c: ${c}
  dt: ${dt}
l2:
  # Which cell to use
  cell: ef_adlif
  input_size: 360
  # number of neurons
  n_neurons: 360

  # Range of tau_u
  tau_u_range: [5, 25]

  # Range of tau_w
  tau_w_range: [60, 300]
  # Factor for reparametrization of a and b (see paper)
  q: 60
  # input size
  alpha: ${alpha}
  c: ${c}
  dt: ${dt}
l_out:
  input_size: 360
  n_neurons: ${dataset.num_classes}
  # Time constant of output layer 
  tau_u_range: [15, 15]
  dt: ${dt}
# Number of epochs
n_epochs: 300

# Batch size
batch_size: 256

# Loss aggregation
loss_agg: softmax

# Metric tracking
tracking_metric: val_acc
tracking_mode: max

# optimizer parameters
# learning rate of the optimizer
lr: 0.01
factor: 0.9
patience: 9999
